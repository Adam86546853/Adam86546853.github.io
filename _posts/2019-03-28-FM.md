---
http://127.0.0.1:4000http://127.0.0.1:4000layout: post
title: FM算法原理
date: 2019-03-27
subtitle: '认识FM'
cover: '/assets/img/fm.jpg'
tags: FM
---



---

### 1.算法原理：



- 在CTR预估中，往往需要进行特征组合，一般的ctr预估，譬如LR线性模型等拟合的都是线性特征，对非线性特征缺乏准确的刻画，特征组合正好可以加入非线性部分，增强模型的表达能力。工业界一般进行特征组合有2大类，一种是FM，另一个是Tree方式。其中FM是直接进行特征交叉的方式进行训练，把交叉特征叫人到特征权重中；Tree Model 是通过组合简单的模型是模型复杂化，简单模型无法拟合非线性函数，通过组合简单模型来增加总模型的复杂度，从而拟合非线性部分。

  下面主要介绍的是FM算法

- **FM的背景来源**

   - FM(Factorization Machine) 解决的是特征稀疏情况下，怎么样组合稀疏特征的问题。 现有的线性模型中，一般都会将特征进行**onehot** 编码，然后输入模型中便于训练。

     ![img](/assets/img/fm1.png)

    clicked是分类值，表明用户有没有点击该广告。1表示点击，0表示未点击。而country,day,ad_type则是对应   的特征。对于这种categorical特征，一般都是进行one-hot编码处理

   ![img](/assets/img/fm2.png)

   因为是categorical特征，所以经过one-hot编码以后，不可避免的样本的数据就变得很稀疏。举个非常简单的例子，假设淘宝或者京东上的item为100万，如果对item这个维度进行one-hot编码，光这一个维度数据的稀疏度就是百万分之一。由此可见，数据的稀疏性，是我们在实际应用场景中面临的一个非常常见的挑战与问题。one-hot编码带来的另一个问题是特征空间变大。同样以上面淘宝上的item为例，将item进行one-hot编码以后，样本空间有一个categorical变为了百万维的数值特征，特征空间一下子暴增一百万。所以大厂动不动上亿维度，就是这么来的。

- **特征组合**

   普通的线性模型，我们都是将各个特征独立考虑的，并没有考虑到特征与特征之间的相互关系。但实际上，大量的特征之间是有关联的。最简单的以电商为例，一般女性用户看化妆品服装之类的广告比较多，而男性更青睐各种球类装备。那很明显，女性这个特征与化妆品类服装类商品有很大的关联性，男性这个特征与球类装备的关联性更为密切。如果我们能将这些有关联的特征找出来，显然是很有意义的

   一般的线性模型为：

   $$y=w_0 + \sum_i^nw_i*x_i$$

   从上面的式子很容易看出，一般的线性模型压根没有考虑特征间的关联。为了表述特征间的相关性，我们采用多项式模型。在多项式模型中，特征xi与xj的组合用xixj表示。为了简单起见，我们讨论二阶多项式模型。具体的模型表达式如下:

   $$y=w_0 + \sum_iw_ix_i + \sum_i^{n-1} \sum_j^{i+1} w_{ij}x_ix_j ​$$

   上式中，n表示样本的特征数量,xi表示第i个特征。
   与线性模型相比，FM的模型就多了后面特征组合的部分。

   从上面的公式可以看出，需要求解的参数为$n/(n-1)/2​$   在数据稀疏的情况下，满足$x_i​$ $x_j​$ 的都不为0的数据非常少，所有较难求解，这里引入辅助向量$V_i=(v_{i1},v_{i2},…v_{in})​$  

   $$V = \begin{bmatrix}  v_{11}&v_{12} & \cdots &v_{1k}  \\  v_{21}&v_{22} & \cdots &v_{2k} \\  \vdots & \vdots & &\vdots \\ v_{n1}&v_{n2} & \cdots &v_{nk}   \end{bmatrix} _{n*k} = \begin{bmatrix} \mathbf v_1 \\ \mathbf v_2\\ \vdots \\ \mathbf v_n\end{bmatrix}$$ 

   $$\hat{W} = VV^T = \begin{bmatrix} \mathbf v_1 \\ \mathbf v_2\\ \vdots \\ \mathbf v_n\end{bmatrix} . \begin{bmatrix} \mathbf v_1 &\mathbf v_2& \cdots & \mathbf v_n\end{bmatrix}$$ 

   所以有 $w_{ij} = <\mathbf v_i,\mathbf v_j>$ 

   **化简非线性部分如下：**

   $\sum_i^{n-1} \sum_j^{i+1} w_{ij}x_ix_j  $   

   $=\sum_i^{n-1} \sum_j^{i+1}<v_i,v_j> x_ix_j​$   半对称矩阵

   $=1/2 \sum_1^{n} \sum_1^{n}<v_i,v_j> x_ix_j  -  1/2\sum_1^{n}<v_i,v_i> x_ix_i  ​$   (全矩阵 - 对角线)/2

   $=1/2(\sum_1^{n} \sum_1^{n}\sum_f^k v_{if}v_{jf}x_ix_j  -  \sum_1^{n}\sum_f^k v_{if}v_{if}x_ix_i)$

   $=1/2\sum_f^k((\sum_1^nv_{if}x_i \sum_1^nv_{jf}x_j ) -  \sum_1^nv_{if}^2x_i^2)​$

   $=1/2\sum_f^k((\sum_1^nv_{if}x_i )^2 -  \sum_1^nv_{if}^2x_i^2)$

   所以非线性部分化简从O(n*n ) 降低 到O(kn) 的复杂度，k<<n 

   通过SGD求解可以得到 

   $$ \frac {\partial y(x)}{\theta}  = \left\{ \begin{array}{ll} 1 & if \;  \theta  \;is \;w_0 \\ x_i & if \;\theta \; is \; w_i \\ x_i \sum_{j=n}^n v_{jf}x_j -v_{if}x_i^2 & if \; \theta  \;is \;v_{jf}   \end{array} \right.  $$

   

### 2.代码与实现



```python
from __future__ import print_function
from itertools import count
from collections import defaultdict
from scipy.sparse import csr
import pandas as pd
import numpy as np
import tensorflow as tf

def vectorize_dic(dic, ix=None, p=None):
    """ 
    Creates a scipy csr matrix from a list of lists (each inner list is a set of values corresponding to a feature) 

    parameters:
    -----------
    dic -- dictionary of feature lists. Keys are the name of features
    ix -- index generator (default None)
    p -- dimension of featrure space (number of columns in the sparse matrix) (default None)
    """
    if (ix == None):
        d = count(0)
        ix = defaultdict(lambda: next(d))

    n = len(list(dic.values())[0])  # num samples
    g = len(list(dic.keys()))  # num groups
    nz = n * g  # number of non-zeros

    col_ix = np.empty(nz, dtype=int)

    i = 0
    for k, lis in dic.items():
        # append index el with k in order to prevet mapping different columns with same id to same index
        col_ix[i::g] = [ix[str(el) + str(k)] for el in lis]
        i += 1

    row_ix = np.repeat(np.arange(0, n), g)
    data = np.ones(nz)

    if (p == None):
        p = len(ix)

    ixx = np.where(col_ix < p)

    return csr.csr_matrix((data[ixx], (row_ix[ixx], col_ix[ixx])), shape=(n, p)), ix

cols = ['user', 'item', 'rating', 'timestamp']
train = pd.read_csv('data/ua.base', delimiter='\t', names=cols)
test = pd.read_csv('data/ua.test', delimiter='\t', names=cols)
dic = {'users': train.user.values, 'items': train.item.values}

X_train, ix = vectorize_dic({'users': train.user.values, 'items': train.item.values})
X_test, ix = vectorize_dic({'users': test.user.values, 'items': test.item.values}, ix, X_train.shape[1])
y_train = train.rating.values
y_test= test.rating.values


X_train = X_train.todense()
X_test = X_test.todense()

# print shape of data
print(X_train.shape)
print(y_train.shape)


import tensorflow as tf

n, p = X_train.shape

# number of latent factors
k = 10

# design matrix
X = tf.placeholder('float', shape=[None, p])
# target vector
y = tf.placeholder('float', shape=[None, 1])

# bias and weights
w0 = tf.Variable(tf.zeros([1]))
W = tf.Variable(tf.zeros([p]))

# interaction factors, randomly initialized
V = tf.Variable(tf.random_normal([k, p], stddev=0.01))

# estimate of y, initialized to 0.
y_hat = tf.Variable(tf.zeros([n, 1]))



linear_terms = tf.add(w0, tf.reduce_sum(tf.multiply(W, X), 1, keep_dims=True))
pair_interactions = (tf.multiply(0.5,
                    tf.reduce_sum(
                        tf.subtract(
                            tf.pow( tf.matmul(X, tf.transpose(V)), 2),
                            tf.matmul(tf.pow(X, 2), tf.transpose(tf.pow(V, 2)))),
                        1, keep_dims=True)))
y_hat = tf.add(linear_terms, pair_interactions)

lambda_w = tf.constant(0.001, name='lambda_w')
lambda_v = tf.constant(0.001, name='lambda_v')

l2_norm = (tf.reduce_sum(
            tf.add(
                tf.multiply(lambda_w, tf.pow(W, 2)),
                tf.multiply(lambda_v, tf.pow(V, 2)))))


error = tf.reduce_mean(tf.square(tf.subtract(y, y_hat)))
loss = tf.add(error, l2_norm)
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(loss)


def batcher(X_, y_=None, batch_size=-1):
    n_samples = X_.shape[0]

    if batch_size == -1:
        batch_size = n_samples
    if batch_size < 1:
       raise ValueError('Parameter batch_size={} is unsupported'.format(batch_size))

    for i in range(0, n_samples, batch_size):
        #upper_bound = min(i + batch_size, n_samples)
        ret_x = X_[i:i+batch_size]
        ret_y = None
        if y_ is not None:
            ret_y = y_[i:i + batch_size]
            yield (ret_x, ret_y)

epochs = 10
batch_size = 1000
init = tf.global_variables_initializer()
sess = tf.Session()
perm = np.random.permutation(X_train.shape[0])
sess.run(init)
# iterate over batches
for bX, bY in batcher(X_train[perm], y_train[perm], batch_size):
    sess.run(optimizer, feed_dict={X: bX.reshape(-1, p), y: bY.reshape(-1, 1)})


errors = []
for bX, bY in batcher(X_test, y_test):
    errors.append(sess.run(error, feed_dict={X: bX.reshape(-1, p), y: bY.reshape(-1, 1)}))

RMSE = np.sqrt(np.array(errors).mean())
print(RMSE)
sess.close()
```



---



在python3上面是可以跑的

数据集需要从<https://grouplens.org/datasets/movielens/100k/> 下载