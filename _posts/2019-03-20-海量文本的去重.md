---
layout: post
title: 海量文本的去重simHash和minHash
date: 2019-03-30
subtitle: 'simHash和minHash'
cover: '/assets/img/lsh.jpg'
tags: simHash,minHash
---



---



### 1.算法：



- **海量文本去重的背景**

   最近做项目的时候，发现需要对海量的文本的去重，首先文本无非就是表示成(word,weight)这样的带权重的词向量方式，也即是词袋模型，这样往往表示的文本维度都非常高，而文本的数量往往也是上百万和千万，网页数量更甚，一般都上亿。所以针对这种高维度和高数量的海量文本，如何快速查重或者找近邻都是很有挑战。如果是低维的小数据集，我们通过线性查找（Linear Search）就可以容易解决，但如果是对一个海量的高维数据集采用线性查找匹配的话，会非常耗时，因此，为了解决该问题，我们需要采用一些类似索引的技术来加快查找过程，通常这类技术称为最近邻查找（Nearest  Neighbor,AN），例如K-d tree(针对KDTRtree会另开一个文章讲)；或近似最近邻查找（Approximate Nearest  Neighbor, ANN），例如K-d tree with BBF, Randomized Kd-trees, Hierarchical K-means Tree。而LSH是ANN中的一类方法。所以一般对于海量文本处理一般都是**文本降维(文本签名)和缩小候选集(hash分桶)**。

- **局部敏感哈希(Locality-Sensitive Hashing, LSH)方法介绍**

   ​       通过建立Hash Table的方式我们能够得到O(1)的查找时间性能，其中关键在于选取一个hash function，将原始数据映射到相对应的桶内（bucket, hash bin），例如对数据求模：h = x mod w，w通常为一个素数。在对数据集进行hash 的过程中，会发生不同的数据被映射到了同一个桶中（即发生了冲突collision），这一般通过再次哈希将数据映射到其他空桶内来解决。这是普通Hash方法或者叫传统Hash方法，LSH则不太一样。

   ​        **LSH的基本思想是:**将原始数据空间中的两个相邻数据点通过相同的映射或投影变换（projection）后，这两个数据点在新的数据空间中仍然相邻的概率很大，而不相邻的数据点被映射到同一个桶的概率很小。也就是说，如果我们对原始数据进行一些hash映射后，我们希望原先相邻的两个数据能够被hash到相同的桶内，具有相同的桶号。对原始数据集合中所有的数据都进行hash映射后，我们就得到了一个hash table，这些原始数据集被分散到了hash table的桶内，每个桶会落入一些原始数据，属于同一个桶内的数据就有很大可能是相邻的，当然也存在不相邻的数据被hash到了同一个桶内。因此，如果我们能够找到这样一些hash functions，使得经过它们的哈希映射变换后，原始空间中相邻的数据落入相同的桶内的话，那么我们在该数据集合中进行近邻查找就变得容易了，我们只需要将查询数据进行哈希映射得到其桶号，然后取出该桶号对应桶内的所有数据，再进行线性匹配即可查找到与查询数据相邻的数据。换句话说，我们通过hash function映射变换操作，将原始数据集合分成了多个子集合，而每个子集合中的数据间是相邻的且该子集合中的元素个数较小，因此将一个在超大集合内查找相邻元素的问题转化为了在一个很小的集合内查找相邻元素的问题，显然计算量下降了很多。

   ​        那具有怎样特点的hash functions才能够使得原本相邻的两个数据点经过hash变换后会落入相同的桶内？这些hash function需要满足以下两个条件：

   1）如果d(x,y) ≤ d1， 则h(x) = h(y)的概率至少为p1；

   2）如果d(x,y) ≥ d2， 则h(x) = h(y)的概率至多为p2；

   其中d(x,y)表示x和y之间的距离，d1 < d2， h(x)和h(y)分别表示对x和y进行hash变换。

   满足以上两个条件的hash functions称为(d1,d2,p1,p2)-sensitive。而通过一个或多个(d1,d2,p1,p2)-sensitive的hash function对原始数据集合进行hashing生成一个或多个hash table的过程称为Locality-sensitive Hashing。直白一点的理解就是跟以前的hash函数不同，以前尽量是减少hash后的冲突，LSH则是在一定条件小保持原有数据的分布，即使经过hash过后，原先相邻的元素还是有很大概率在一个hash桶内。

- **介绍一些满足不同距离度量方式下的locality-sensitive的hash functions：**

  - **Jaccard distance (minHash)**

    Jaccard distance： (1 - Jaccard similarity)，而Jaccard similarity = (A intersection B) / (A union B)，Jaccard similarity通常用来判断两个集合的相似性。Jaccard distance对应的LSH hash function为：**minhash**，其是(d1,d2,1-d1,1-d2)-sensitive的

  - **Cosine distance (simHash基于随机超平面演变)**

    Cosine distance：cos(theta) = A·B / A B ，常用来判断两个向量之间的夹角，夹角越小，表示它们越相似。Cosine distance对应的LSH hash function为：H(V) = sign(V·R)，R是一个随机向量。V·R可以看做是将V向R上进行投影操作。其是(d1,d2,(180-d1)180,(180-d2)/180)-sensitive的。

  - **Hamming distance**(simHash 指纹可以根据汉明距离计算距离,一般<3)

    Hamming distance： 两个具有相同长度的向量中对应位置处值不同的次数。Hamming distance对应的LSH hash function为：H(V) = 向量V的第i位上的值，其是(d1,d2,1-d1/d,1-d2/d)-sensitive

  - **normal Euclidean distance**

    Euclidean distance是衡量D维空间中两个点之间的距离的一种距离度量方式。Euclidean distance对应的LSH hash function为：H(V) = V·R + b / a，R是一个随机向量，a是桶宽，b是一个在[0,a]之间均匀分布的随机变量。V·R可以看做是将V向R上进行投影操作。其是(a/2,2a,1/2,1/3)-sensitive的。理解：将原始数据空间中的数据投影到一条随机的直线（random line）上，并且该直线由很多长度等于a的线段组成，每一个数据被投影后会落入该直线上的某一个线段上（对应的桶内），将所有数据都投影到直线上后，位于同一个线段内的数据将被认为具有很大可能是相邻的（即原始数据之间的Euclidean distance很小)

- **常用的增强LSH的方法：**

  - 使用多个独立的hash table

    每个hash table由k个LSH hash function创建，每次选用k个LSH hash function（同属于一个LSH function family）就得到了一个hash table，重复多次，即可创建多个hash table。多个hash table的好处在于能够降低false positive rate.

  - AND 与操作

    从同一个LSH function family中挑选出k个LSH function，H(X) = H(Y)有且仅当这k个Hi(X) = Hi(Y)都满足。也就是说只有当两个数据的这k个hash值都对应相同时，才会被投影到相同的桶内，只要有一个不满足就不会被投影到同一个桶内。AND与操作能够使得找到近邻数据的p1概率保持高概率的同时降低p2概率，即降低了falsenegtiverate。

  - OR 或操作

    从同一个LSH function family中挑选出k个LSH function，H(X) = H(Y)有且仅当存在一个以上的Hi(X) = Hi(Y)。也就是说只要两个数据的这k个hash值中有一对以上相同时，就会被投影到相同的桶内，只有当这k个hash值都不相同时才不被投影到同一个桶内。OR或操作能够使得找到近邻数据的p1概率变的更大（越接近1）的同时保持p2概率较小，即降低了false positive rate。

  - AND和OR的级联

    将与操作和或操作级联在一起，产生更多的hahs table，这样的好处在于能够使得p1更接近1，而p2更接近0

- **simHash和minHash**

  - **minHash**

    1.采用jaccard相似度

    <http://www.cnblogs.com/bourneli/archive/2013/04/04/2999767.html>   这里写的不错，

  - simHash

    ![img](/assets/img/simhash.jpg)

    如何将其扩展到海量数据的近重复检测中去呢？譬如说对于64位的待查询文本的simhash code来说，如何在海量的样本库（>1M）中查询与其海明距离在3以内的记录呢？下面在引入simhash的索引结构之前，先提供两种常规的思路。(无非就是时间和空间的平衡)

    1. 耗时间(针对每个simhash都进行查询，因为只有3个位不同，需要$C_{64}^3$ 次查询，然后找出候选集 )
    2. 耗空间(针对每个simhash，保存该simhash3个位不同的候选集)，空间要求比较大，但只需要查询一次，
    3. 折中办法，将simhash分成4段(根据抽柜原理，分成4段，总有1段是相同的)，保存分成的4段的索引，每段16位，索引有$2^{16}$ 个，每个索引下最多有$2^{64-16}$ 个(实际上样本没这么多，10亿样本$2^{34}$ ，则每个索引下 $2^{34-16} = 262144$ 个样本)


