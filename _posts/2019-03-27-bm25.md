---
layout: post
title: BM25算法
date: 2018-10-17
subtitle: 'bm25'
cover: '/assets/img/bm25.jpg'
tags: BM25
---

# BM25算法



1. ###原理

   - **概括来讲就是分解$Query$ ，然后分解后生成各个元素$q_i$ ，然后根据$q_i$ 和搜索的结果文档D 进行一个加权求和**

     $Score(Q,d)=\sum_{i}^n{W_i*R(q_i,d_i)}$    

     其中 $W_i​$ 表示权重，$R(q_i,d_i)​$ 表示 查询的$Query​$ 的元素$q_i​$ 和 文档 $d_i​$  之间的相关性。词和文档的权重可以使用IDF来计算:

     $W_i=log{ N-n(q_i) +0.5  \over n(q_i)+0.5} ​$    

       $n(q_i)$  : 包含词 $q_i$ 的文档个数，  N : 总文档数  ， 0.5系数 平滑处理 ，$W_i$  描述如果包含$q_i$ 的文档数越多，则$W_i$  就越小，权重就越小，该元素$q_i$ 的文档间区分度就不高

   -  **再来看$R(q_i,d)$ 相关性公式**

     $R(q_i,d)={f_i*(k_1 +1 ) \over f_1 +K} * {qf_i*(k_2 +1) \over qf_i + k_2 }$            公式1

     $K=k_1*(1-b+b*{dl \over avgdl})$                 公式2

     其中， $k_1 k_2 b$  为调节因子，可以根据经验设置， 一般有 $k_1=2   , b=0.75$  ；$f_i$  为 $q_i$  在文档 $d$  中出现的词频， $qf_i$  为 $ q_i  在 Query $   中出现的词频，一般在 $Query$ 中只出现一次，所以 $qf_i$ =1  

     则简化相关性公式为：

     $R(q_i,d) = {f_i*(k_1 +1 ) \over f_1 +K}​$                                公式3

     其中K反应的是文档长短的影响因子，b是决定文档长度影响程度的一个参数，dl 越大，则K越大，

     $R(q_i,d)$ 越小，即文档长度越长，越容易包含$q_i$ ,影响度就不高。

   -  **最后，根据公式1和3进行合并，可以得到最后的公式**

      $Score(Q,d) = \sum_i^n {log{ N-n(q_i) +0.5  \over n(q_i)+0.5} } * {f_i*(k_1 +1 ) \over f_1 +K} $

     

     

2. ### 代码与实现

   ```python
   #coding:utf-8
   
   import  math
   import jieba
   import  re
   import sys
   
   if sys.getdefaultencoding() != 'utf-8':
       reload(sys)
       sys.setdefaultencoding('utf-8')
       print sys.getdefaultencoding()
   
   text =  """
   自然语言处理是计算机科学领域与人工智能领域中的一个重要方向。
   它研究能实现人与计算机之间用自然语言进行有效通信的各种理论和方法。
   自然语言处理是一门融语言学、计算机科学、数学于一体的科学。
   因此，这一领域的研究将涉及自然语言，即人们日常使用的语言，
   所以它与语言学的研究有着密切的联系，但又有重要的区别。
   自然语言处理并不是一般地研究自然语言，
   而在于研制能有效地实现自然语言通信的计算机系统，
   特别是其中的软件系统。因而它是计算机科学的一部分。
   """
   
   
   class BM25(object):
       def __init__(self,docs):
           self.D=len(docs)   #文档长度
           self.avgdl=sum([len(doc) for doc in docs])*1.0/self.D
           self.docs=docs
           self.f=[] # 每个元素是一个字典，存储的是文档中词出现的频率
           self.df={}  #每个词，出现在文档的个数
           self.idf={} # 每个词的idf值
           self.k1=1.5
           self.b=0.5
           self.init()
   
       def init(self):
           for doc in self.docs :
               tmp={}
               for word in doc:
                   tmp[word]=tmp.get(word,0)+1
               self.f.append(tmp)
   
               for k in tmp.keys():
                   self.df[k]=self.df.get(k,0)+1
   
           for k,v in self.df.items():
               self.idf[k]=math.log(self.D-v+0.5) - math.log(v+0.5)
   
   
       def sim(self,doc,index):
           score=0
           for word in doc:
               if word not in self.f[index]:
                   print word,'not in ',self.f[index]
                   continue
               d=len(self.f[index])
               f=self.f[index][word]
               k=self.k1*(1 - self.b + self.b * d*1.0/ self.avgdl)
               print self.idf[word]
   
               score+=self.idf[word]*f*(self.k1+1)*1.0/(f+k)
           return score
   
       def simAll(self,doc):
           scores=[]
           for index in range(self.D):
               score = self.sim(doc,index)
               scores.append(score)
   
           return scores
   
   def stopwordslist(filepath):
       stopwords = [line.strip() for line in open(filepath, 'r').readlines()]
       return stopwords
   
   # 对句子去除停用词
   def movestopwords(sentence):
       stopwords = stopwordslist('/Users/dlv/Desktop/stopwords_cn.txt')  # 这里加载停用词的路径
       out = []
       for word in sentence:
           if word not in stopwords:
               if word != '\t'and'\n':
                   out.append(word)
       return out
   
   
   if __name__ == '__main__':
       sentences = re.split('。|！|\!|\.|？|\?|\\n', text.decode("utf-8"))  # 保留分割符
       docs=[]
       for sentence in sentences:
           words = list(jieba.cut(sentence))
           words = movestopwords(words)
           if words:
               docs.append(words)
       s=BM25(docs)
   
       print(s.sim(['自然语言'.decode('utf-8')],0))
   
   ```





>  这个是python2.7 下能够跑的，2.7对中文支持的不是很好。

